services:
    llm-experts-devcontainer:
        network_mode: host
        image: llm-experts-devcontainer
        container_name: llm-experts-devcontainer
        build:
            context: ..
            dockerfile: .devcontainer/Dockerfile
            args:
                - PYTHON_VERSION
                - DEVCONTAINER_USER
        volumes:
            - ..:/workspace:cached
            - ../resources/:/resources
        env_file:
            - ../.secrets/.env
        # This keeps the devcontainer running.
        entrypoint: [ "tail", "-f", "/dev/null" ]

    llm-experts-redis:
        image: redis:${REDIS_VERSION}
        container_name: llm-experts-redis
        ports:
            - "6379:6379"
        volumes:
            - $PWD/resources/cache/redis:/data

    llm-experts-mongo:
        image: mongo:${MONGODB_VERSION}
        container_name: llm-experts-mongo
        ports:
            - "127.0.0.1:27017:27017"
        volumes:
            - $PWD/resources/db/mongo/db:/data/db:z
        deploy:
            resources:
                limits:
                    cpus: "1"
                    memory: 1G

    llm-experts-ollama:
        image: ollama/ollama
        container_name: llm-experts-ollama
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: 1
                          capabilities: [ gpu ]
        ports:
            - "127.0.0.1:11434:11434"
        volumes:
            - $PWD/resources/ollama:/root/.ollama
